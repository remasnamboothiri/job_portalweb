// Initialize everything with proper sequencing and debugging
    document.addEventListener('DOMContentLoaded', async function() {
        debugLog('üöÄ DOM LOADED - Starting enhanced initialization sequence...', 'info');
        
        // Validate template data first
        debugLog('üìã TEMPLATE VALIDATION:', 'info');
        debugLog(`- Question available: ${TEMPLATE_DATA.initialQuestion ? 'YES' : 'NO'} (${TEMPLATE_DATA.initialQuestion.length} chars)`, 'info');
        debugLog(`- Audio available: ${TEMPLATE_DATA.initialAudio ? 'YES' : 'NO'} (${TEMPLATE_DATA.initialAudio})`, 'info');
        debugLog(`- Duration: ${TEMPLATE_DATA.initialDuration}s`, 'info');
        debugLog(`- Has Audio flag: ${TEMPLATE_DATA.hasAudio}`, 'info');
        
        try {
            // STEP 1: Initialize camera and microphone (CRITICAL)
            debugLog('üéØ STARTING STEP 1: Camera and Microphone initialization...', 'info');
            const cameraSuccess = await initCamera();
            
            if (!cameraSuccess) {
                debugLog('‚ùå CRITICAL FAILURE: Camera/microphone initialization failed', 'error');
                debugLog('‚ùå Cannot proceed without microphone access', 'error');
                return;
            }
            
            debugLog('‚úÖ STEP 1 COMPLETE: Camera and microphone ready', 'success');
            
            // STEP 2: Wait and initialize speech recognition
            debugLog('üéØ STARTING STEP 2: Speech recognition initialization (2 second delay)...', 'info');
            await new Promise(resolve => {
                setTimeout(() => {
                    const speechSuccess = initSpeech();
                    if (speechSuccess) {
                        debugLog('‚úÖ STEP 2 COMPLETE: Speech recognition initialized', 'success');
                    } else {
                        debugLog('‚ùå STEP 2 FAILED: Speech recognition initialization failed', 'error');
                    }
                    resolve();
                }, 2000);
            });
            
            // STEP 3: Start microphone with delay
            debugLog('üéØ STARTING STEP 3: Microphone activation (1 second delay)...', 'info');
            setTimeout(() => {
                const micSuccess = startMicrophone();
                if (micSuccess) {
                    debugLog('‚úÖ STEP 3 COMPLETE: Microphone activated and listening', 'success');
                } else {
                    debugLog('‚ùå STEP 3 FAILED: Microphone activation failed', 'error');
                }
            }, 1000);
            
            // STEP 4: Start timer
            debugLog('üéØ STARTING STEP 4: Timer initialization...', 'info');
            setInterval(updateTimer, 1000);
            debugLog('‚úÖ STEP 4 COMPLETE: Timer started', 'success');
            
            // STEP 5: Show initial question with delay
            debugLog('üéØ STARTING STEP 5: Initial question display (4 second delay)...', 'info');
            setTimeout(() => {
                debugLog('üìù Preparing to show initial question...', 'info');
                
                const question = TEMPLATE_DATA.initialQuestion || "Welcome to your AI interview! Let's begin with your introduction.";
                const audioUrl = TEMPLATE_DATA.initialAudio || null;
                const duration = TEMPLATE_DATA.initialDuration || 5;
                
                if (question.length < 10) {
                    debugLog('‚ö†Ô∏è Question too short, using enhanced fallback', 'warning');
                    const fallbackQuestion = `Hello ${TEMPLATE_DATA.candidateName}! Welcome to your AI interview. I'm excited to learn about your background and experience. Could you please start by telling me about yourself, your education, and what interests you most about this position?`;
                    showCurrentQuestion(fallbackQuestion, audioUrl, duration);
                } else {
                    showCurrentQuestion(question, audioUrl, duration);
                }
                
                debugLog('‚úÖ STEP 5 COMPLETE: Initial question displayed', 'success');
            }, 4000);
            
            debugLog('‚úÖ ALL STEPS COMPLETED: Enhanced interview system fully initialized!', 'success');
            debugLog('üé§ System should now be listening for candidate speech...', 'success');
            
        } catch (error) {
            debugLog(`‚ùå CRITICAL ERROR during initialization: ${error.message}`, 'error');
            debugLog(`Error stack: ${error.stack}`, 'error');
            
            document.getElementById('conversationArea').innerHTML = 
                `<div style="color: #f44336; padding: 20px; text-align: center;">
                    <h3>System Initialization Failed</h3>
                    <p>Critical error during interview system setup.</p>
                    <p>Error: ${error.message}</p>
                    <button onclick="location.reload()" style="background: #1a73e8; color: white; border: none; padding: 10px 20px; border-radius: 5px; cursor: pointer; margin-top: 15px;">
                        Restart Interview
                    </button>
                </div>`;
        }
    });
    
    // Enhanced error handling
    window.addEventListener('error', function(e) {
        debugLog(`JavaScript error: ${e.message} at ${e.filename}:${e.lineno}`, 'error');
        console.error('Global error:', e);
    });
    
    window.addEventListener('unhandledrejection', function(e) {
        debugLog(`Unhandled promise rejection: ${e.reason}`, 'error');
        console.error('Unhandled rejection:', e);
    });
    
    // Page visibility handler - restart microphone when page becomes visible
    document.addEventListener('visibilitychange', function() {
        if (document.hidden) {
            debugLog('Page hidden - pausing microphone', 'warning');
        } else {
            debugLog('Page visible - checking microphone state', 'info');
            // Restart microphone if it should be running
            setTimeout(() => {
                if (!isInterviewerSpeaking && !isProcessingResponse && !interviewCompleted && !isListening) {
                    debugLog('Restarting microphone after page visibility change', 'info');
                    startMicrophone();
                }
            }, 1000);
        }
    });
    
    // Add manual microphone test function
    window.testMicrophone = function() {
        debugLog('MANUAL MICROPHONE TEST INITIATED', 'info');
        
        if (!recognition) {
            debugLog('No recognition object available for testing', 'error');
            return;
        }
        
        // Stop current recognition
        if (isListening) {
            try {
                recognition.stop();
                isListening = false;
            } catch (e) {
                debugLog('Error stopping current recognition for test', 'warning');
            }
        }
        
        setTimeout(() => {
            debugLog('Starting manual microphone test...', 'info');
            startMicrophone();
        }, 1000);
    };
    
    console.log('Enhanced Interview System Loaded');
    console.log('Available debugging commands:');
    console.log('- testMicrophone() - Test microphone manually');
    console.log('- Check console for detailed logs');
    
    </script>
</body>
</html>
                {% load static %}
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Interview - {{ candidate_name }}</title>
    <link rel="stylesheet" href="{% static 'css/professional-interview.css' %}">
    <style>
        body {
            background: #202124;
            color: white;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 20px;
            height: 100vh;
            overflow: hidden;
        }
        .container {
            max-width: 1400px;
            margin: 0 auto;
            height: calc(100vh - 40px);
            display: flex;
            flex-direction: column;
        }
        .video-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin-bottom: 20px;
        }
        .video-box {
            background: #303134;
            border-radius: 12px;
            height: 300px;
            display: flex;
            align-items: center;
            justify-content: center;
            position: relative;
            border: 2px solid #444;
        }
        .ai-avatar {
            font-size: 80px;
            animation: pulse 2s ease-in-out infinite alternate;
        }
        .ai-speaking {
            animation: pulse 1s ease-in-out infinite alternate;
            border-color: #ff9800 !important;
            box-shadow: 0 0 20px rgba(255, 152, 0, 0.3);
        }
        .candidate-speaking {
            border-color: #4caf50 !important;
            box-shadow: 0 0 20px rgba(76, 175, 80, 0.3);
        }
        video {
            width: 100%;
            height: 100%;
            object-fit: cover;
            border-radius: 12px;
        }
        .participant-name {
            position: absolute;
            bottom: 16px;
            left: 16px;
            background: rgba(0,0,0,0.8);
            padding: 8px 16px;
            border-radius: 20px;
            font-size: 16px;
            font-weight: bold;
            backdrop-filter: blur(10px);
        }
        .controls {
            position: fixed;
            bottom: 20px;
            left: 50%;
            transform: translateX(-50%);
            text-align: center;
            z-index: 1000;
            display: flex;
            gap: 15px;
        }
        .btn {
            background: #34a853;
            color: white;
            border: none;
            padding: 15px 20px;
            border-radius: 50px;
            font-size: 24px;
            cursor: pointer;
            transition: all 0.3s ease;
            min-width: 60px;
            height: 60px;
            display: flex;
            align-items: center;
            justify-content: center;
        }
        .btn:hover {
            transform: scale(1.05);
            box-shadow: 0 4px 15px rgba(0,0,0,0.3);
        }
        .btn.recording {
            background: #ea4335;
            animation: recording-pulse 1s infinite;
        }
        .btn.disabled {
            background: #666;
            cursor: not-allowed;
            opacity: 0.5;
            pointer-events: none;
        }
        .btn.end-call {
            background: #ea4335;
        }
        .transcript-section {
            background: #303134;
            padding: 25px;
            border-radius: 12px;
            margin: 20px 0;
            flex: 1;
            display: flex;
            flex-direction: column;
            border: 2px solid #444;
            min-height: 300px;
        }
        .conversation-text {
            background: transparent;
            padding: 20px;
            border-radius: 8px;
            color: white;
            font-size: 18px;
            line-height: 1.6;
            text-align: center;
            flex: 1;
            overflow-y: auto;
        }
        .debug-panel {
            background: #1a1a1a;
            padding: 15px;
            border-radius: 8px;
            margin-top: 10px;
            font-size: 12px;
            font-family: 'Courier New', monospace;
            color: #90a4ae;
            max-height: 150px;
            overflow-y: auto;
            display: block; /* Show during debugging */
        }
        .debug-panel .status {
            margin: 5px 0;
            padding: 3px 8px;
            border-radius: 4px;
        }
        .debug-panel .status.success { background: rgba(76, 175, 80, 0.2); color: #81c784; }
        .debug-panel .status.warning { background: rgba(255, 152, 0, 0.2); color: #ffb74d; }
        .debug-panel .status.error { background: rgba(244, 67, 54, 0.2); color: #e57373; }
        .debug-panel .status.info { background: rgba(33, 150, 243, 0.2); color: #64b5f6; }
        
        .stats {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 20px;
            padding: 0 10px;
        }
        .stat {
            background: #303134;
            padding: 12px 24px;
            border-radius: 25px;
            font-weight: 500;
            border: 2px solid #444;
        }
        .audio-status {
            position: fixed;
            top: 20px;
            left: 50%;
            transform: translateX(-50%);
            background: #1a73e8;
            color: white;
            padding: 12px 20px;
            border-radius: 25px;
            font-size: 14px;
            font-weight: 500;
            z-index: 1000;
            display: none;
            box-shadow: 0 4px 15px rgba(0,0,0,0.3);
        }
        .audio-status.playing {
            display: block;
            background: #ff9800;
            animation: status-pulse 2s ease-in-out infinite;
        }
        .audio-status.listening {
            display: block;
            background: #4caf50;
            animation: status-pulse 2s ease-in-out infinite;
        }
        
        /* Animations */
        @keyframes pulse {
            0%, 100% { opacity: 0.8; }
            50% { opacity: 1; }
        }
        @keyframes recording-pulse {
            0%, 100% { transform: scale(1); box-shadow: 0 0 0 0 rgba(234, 67, 53, 0.7); }
            50% { transform: scale(1.05); box-shadow: 0 0 0 10px rgba(234, 67, 53, 0); }
        }
        @keyframes status-pulse {
            0%, 100% { opacity: 0.9; }
            50% { opacity: 1; }
        }
    </style>
</head>
<body>
    <div class="container">
        <!-- Audio status indicator -->
        <div class="audio-status" id="audioStatus"></div>
        
        <div class="stats">
            <div class="stat">Question: <span id="questionCount">1/8</span></div>
            <div class="stat">Time: <span id="timer">15:00</span></div>
        </div>
        
        <div class="video-grid">
            <div class="video-box" id="aiVideoBox">
                <div class="ai-avatar" id="aiAvatar">ü§ñ</div>
                <div class="participant-name">Alex - AI Interviewer</div>
            </div>
            <div class="video-box" id="candidateVideoBox">
                <video id="userVideo" autoplay muted playsinline></video>
                <div class="participant-name">{{ candidate_name|default:"Candidate" }}</div>
            </div>
        </div>

        <div class="transcript-section">
            <div class="conversation-text" id="conversationArea">
                Welcome! Preparing your interview...
            </div>
            
            <!-- DEBUG PANEL - Hidden by default -->
            <div class="debug-panel" id="debugPanel">
                <strong>Debug Log:</strong><br>
                <div id="debugLog">System starting...</div>
            </div>
        </div>

        <div class="controls">
            <button class="btn recording" id="micBtn" onclick="toggleMic()" title="Microphone">üé§</button>
            <button class="btn" id="cameraBtn" onclick="toggleCamera()" title="Camera">üìπ</button>
            <button class="btn end-call" id="endCallBtn" onclick="endInterview()" title="End Interview">üìû</button>
        </div>
    </div>

    <!-- Audio element for playing interviewer's voice -->
    <audio id="aiAudio" preload="auto" style="display: none;"></audio>
    
    <!-- CSRF Token -->
    {% csrf_token %}
    
    <!-- Load typewriter script -->
    <script src="{% static 'js/typewriter-sync.js' %}"></script>

    <script>
    // Template data passed from Django
    const TEMPLATE_DATA = {
        initialQuestion: `{{ ai_question|escapejs|default:"Welcome to your AI interview! Let's begin." }}`,
        initialAudio: `{{ audio_url|escapejs|safe }}`,
        initialDuration: {{ audio_duration|default:5 }},
        candidateName: `{{ candidate_name|escapejs|default:"Candidate" }}`,
        interviewUuid: `{{ interview.uuid|default:"" }}`,
        hasAudio: {{ has_audio|yesno:"true,false" }},
        csrfToken: `{{ csrf_token }}`
    };
    
    // Global variables
    let recognition = null;
    let isListening = false;
    let questionCount = 1;
    let timeLeft = 15 * 60;
    let userStream = null;
    let isCameraOn = true;
    let collectedText = '';
    let speechTimeout = null;
    let isProcessingResponse = false;
    let isInterviewerSpeaking = false;
    let interviewCompleted = false;
    let audioEndedTimeout = null;
    let currentAudioElement = null;
    
    // DEBUG: Enhanced logging (can be disabled by setting debug panel to display:none)
    function debugLog(message, type = 'info') {
        const timestamp = new Date().toLocaleTimeString();
        const logElement = document.getElementById('debugLog');
        const statusClass = type;
        
        console.log(`[${timestamp}] ${message}`);
        
        // Only show debug panel if it's visible
        if (logElement && logElement.parentElement.style.display !== 'none') {
            const logEntry = document.createElement('div');
            logEntry.className = `status ${statusClass}`;
            logEntry.innerHTML = `<span style="color: #666;">${timestamp}</span> ${message}`;
            
            logElement.appendChild(logEntry);
            logElement.scrollTop = logElement.scrollHeight;
            
            // Keep only last 20 entries
            while (logElement.children.length > 20) {
                logElement.removeChild(logElement.firstChild);
            }
        }
    }
    
    debugLog('AI Interview System Starting...', 'info');
    
    // Enhanced camera initialization with explicit microphone access
    async function initCamera() {
        try {
            debugLog('STEP 1: Requesting camera and microphone permissions...', 'info');
            
            // First, check if permissions are already granted
            const permissions = await navigator.permissions.query({name: 'microphone'});
            debugLog(`Current microphone permission: ${permissions.state}`, 'info');
            
            userStream = await navigator.mediaDevices.getUserMedia({ 
                video: {
                    width: { ideal: 1280 },
                    height: { ideal: 720 },
                    facingMode: 'user'
                }, 
                audio: {
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true,
                    sampleRate: 44100
                }
            });
            
            const videoElement = document.getElementById('userVideo');
            videoElement.srcObject = userStream;
            
            const audioTracks = userStream.getAudioTracks();
            const videoTracks = userStream.getVideoTracks();
            
            debugLog(`SUCCESS: Media access granted`, 'success');
            debugLog(`Video tracks: ${videoTracks.length}`, 'info');
            debugLog(`Audio tracks: ${audioTracks.length}`, 'info');
            
            if (audioTracks.length > 0) {
                const track = audioTracks[0];
                debugLog(`Microphone: "${track.label}"`, 'info');
                debugLog(`Microphone enabled: ${track.enabled}`, 'info');
                debugLog(`Microphone ready state: ${track.readyState}`, 'info');
                debugLog(`Microphone constraints: ${JSON.stringify(track.getConstraints())}`, 'info');
            } else {
                debugLog('WARNING: No audio tracks found!', 'error');
                throw new Error('No microphone access - audio tracks missing');
            }
            
            return true;
            
        } catch (error) {
            debugLog(`CAMERA/MIC ERROR: ${error.name} - ${error.message}`, 'error');
            
            let message = 'Camera and Microphone Required';
            let details = 'Please allow access to continue with the interview.';
            
            if (error.name === 'NotAllowedError') {
                details = 'Permission denied. Please click "Allow" when prompted and refresh.';
            } else if (error.name === 'NotFoundError') {
                details = 'No camera or microphone found. Please connect devices and refresh.';
            } else if (error.name === 'NotReadableError') {
                details = 'Camera/microphone is already in use by another application.';
            } else {
                details = `Error: ${error.message}`;
            }
            
            document.getElementById('conversationArea').innerHTML = 
                `<div style="color: #f44336; padding: 20px; text-align: center;">
                    <h3>${message}</h3>
                    <p>${details}</p>
                    <button onclick="location.reload()" style="background: #1a73e8; color: white; border: none; padding: 10px 20px; border-radius: 5px; cursor: pointer; margin-top: 15px;">
                        Grant Access & Retry
                    </button>
                </div>`;
            return false;
        }
    }
    
    // ENHANCED: Speech recognition with aggressive debugging and fixes
    function initSpeech() {
        debugLog('STEP 2: Initializing speech recognition...', 'info');
        
        // Check browser support with detailed logging
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        
        if (!SpeechRecognition) {
            debugLog('FATAL: Speech recognition not supported in this browser', 'error');
            debugLog(`User agent: ${navigator.userAgent}`, 'info');
            document.getElementById('conversationArea').innerHTML = 
                `<div style="color: #f44336; padding: 20px; text-align: center;">
                    <h3>Speech Recognition Not Supported</h3>
                    <p>This browser doesn't support speech recognition.</p>
                    <p>Please use Chrome, Edge, or Safari.</p>
                    <p>Current browser: ${navigator.userAgent.split(' ').pop()}</p>
                </div>`;
            return false;
        }
        
        debugLog('Speech Recognition API available', 'success');
        
        try {
            recognition = new SpeechRecognition();
            debugLog('SpeechRecognition object created successfully', 'success');
            
            // Enhanced configuration
            recognition.continuous = true;
            recognition.interimResults = true;
            recognition.lang = 'en-US';
            recognition.maxAlternatives = 1;
            
            debugLog('Speech recognition configured:', 'info');
            debugLog(`- continuous: ${recognition.continuous}`, 'info');
            debugLog(`- interimResults: ${recognition.interimResults}`, 'info');
            debugLog(`- lang: ${recognition.lang}`, 'info');
            
            // ENHANCED EVENT HANDLERS with detailed logging
            recognition.onstart = function() {
                isListening = true;
                document.getElementById('micBtn').classList.add('recording');
                document.getElementById('candidateVideoBox').classList.add('candidate-speaking');
                debugLog('‚úÖ SPEECH RECOGNITION STARTED SUCCESSFULLY', 'success');
                debugLog('Microphone is now actively listening...', 'success');
            };
            
            recognition.onend = function() {
                document.getElementById('candidateVideoBox').classList.remove('candidate-speaking');
                debugLog('‚ö†Ô∏è Speech recognition ended', 'warning');
                
                // Auto-restart logic with detailed logging
                if (isListening && !interviewCompleted && !isInterviewerSpeaking) {
                    debugLog('Conditions met for restart:', 'info');
                    debugLog(`- isListening: ${isListening}`, 'info');
                    debugLog(`- interviewCompleted: ${interviewCompleted}`, 'info');
                    debugLog(`- isInterviewerSpeaking: ${isInterviewerSpeaking}`, 'info');
                    
                    setTimeout(() => {
                        if (recognition && isListening) {
                            try {
                                debugLog('üîÑ RESTARTING speech recognition...', 'info');
                                recognition.start();
                            } catch (e) {
                                debugLog(`‚ùå Restart failed: ${e.message}`, 'error');
                                debugLog('Will retry in 2 seconds...', 'warning');
                                setTimeout(() => {
                                    try {
                                        recognition.start();
                                    } catch (e2) {
                                        debugLog(`‚ùå Second restart failed: ${e2.message}`, 'error');
                                    }
                                }, 2000);
                            }
                        } else {
                            debugLog('‚ùå Cannot restart - conditions not met', 'error');
                        }
                    }, 100);
                } else {
                    debugLog('‚ùå Not restarting speech recognition - conditions not met', 'warning');
                }
            };
            
            recognition.onerror = function(event) {
                debugLog(`‚ùå SPEECH RECOGNITION ERROR: ${event.error}`, 'error');
                document.getElementById('candidateVideoBox').classList.remove('candidate-speaking');
                
                switch(event.error) {
                    case 'not-allowed':
                        debugLog('‚ùå CRITICAL: Microphone access denied by user', 'error');
                        document.getElementById('conversationArea').innerHTML = 
                            `<div style="color: #f44336; padding: 20px; text-align: center;">
                                <h3>‚ùå Microphone Access Denied</h3>
                                <p>Speech recognition requires microphone access.</p>
                                <p>Please:</p>
                                <ul style="text-align: left; max-width: 400px; margin: 0 auto;">
                                    <li>Check the microphone icon in your browser's address bar</li>
                                    <li>Click "Allow" when prompted for microphone access</li>
                                    <li>Refresh the page and try again</li>
                                </ul>
                                <button onclick="location.reload()" style="background: #1a73e8; color: white; border: none; padding: 10px 20px; border-radius: 5px; cursor: pointer; margin-top: 15px;">
                                    Grant Access & Retry
                                </button>
                            </div>`;
                        break;
                    case 'no-speech':
                        debugLog('‚ö†Ô∏è No speech detected - this is normal', 'warning');
                        break;
                    case 'audio-capture':
                        debugLog('‚ùå CRITICAL: Microphone unavailable or in use', 'error');
                        break;
                    case 'network':
                        debugLog('‚ùå Network error during speech recognition', 'error');
                        break;
                    case 'aborted':
                        debugLog('‚ö†Ô∏è Speech recognition aborted', 'warning');
                        break;
                    default:
                        debugLog(`‚ùå Unknown error: ${event.error}`, 'error');
                }
            };
            
            recognition.onresult = function(event) {
                debugLog(`üé§ SPEECH RESULT EVENT FIRED (${event.results.length} results)`, 'success');
                
                let interimTranscript = '';
                let finalTranscript = '';
                
                // Process all results
                for (let i = event.resultIndex; i < event.results.length; i++) {
                    const result = event.results[i];
                    const transcript = result[0].transcript;
                    const confidence = result[0].confidence;
                    
                    debugLog(`Result ${i}: "${transcript}" (final: ${result.isFinal}, confidence: ${confidence?.toFixed(2) || 'N/A'})`, 'info');
                    
                    if (result.isFinal) {
                        finalTranscript += transcript;
                    } else {
                        interimTranscript += transcript;
                    }
                }
                
                // Check if we should process the speech
                const canProcessSpeech = !isInterviewerSpeaking && !isProcessingResponse && !interviewCompleted;
                debugLog(`Can process speech: ${canProcessSpeech}`, 'info');
                debugLog(`- isInterviewerSpeaking: ${isInterviewerSpeaking}`, 'info');
                debugLog(`- isProcessingResponse: ${isProcessingResponse}`, 'info');
                debugLog(`- interviewCompleted: ${interviewCompleted}`, 'info');
                
                if (canProcessSpeech) {
                    // Show live transcription
                    const displayText = collectedText + finalTranscript + 
                        (interimTranscript ? `<span style="opacity: 0.7; font-style: italic; color: #90a4ae;">${interimTranscript}</span>` : '');
                    
                    if (displayText.trim()) {
                        debugLog(`üí¨ Displaying speech: "${displayText.replace(/<[^>]*>/g, '')}"`, 'success');
                        document.getElementById('conversationArea').innerHTML = 
                            `<div style="color: #81c784; font-style: italic; text-align: left; padding: 20px;">
                                <strong>You're saying:</strong><br>
                                ${displayText}
                            </div>`;
                    }
                
                    // Handle final transcript
                    if (finalTranscript.trim()) {
                        collectedText += finalTranscript;
                        debugLog(`üìù Added to collected text. Total length: ${collectedText.length}`, 'success');
                        debugLog(`üìù Current collected text: "${collectedText}"`, 'info');
                    
                        // Clear existing timeout
                        if (speechTimeout) {
                            clearTimeout(speechTimeout);
                            debugLog('‚è±Ô∏è Cleared previous speech timeout', 'info');
                        }
                    
                        // Set new timeout for auto-submission
                        speechTimeout = setTimeout(() => {
                            if (collectedText.trim() && !isProcessingResponse && !isInterviewerSpeaking && !interviewCompleted) {
                                debugLog('‚ö° AUTO-SUBMITTING RESPONSE after pause', 'info');
                                autoSubmitResponse();
                            } else {
                                debugLog('‚ö†Ô∏è Auto-submit conditions not met at timeout', 'warning');
                            }
                        }, 2500);
                        
                        debugLog('‚è±Ô∏è Set new speech timeout (2.5 seconds)', 'info');
                    }
                } else {
                    debugLog('üö´ Speech ignored - system not ready for candidate input', 'warning');
                }
            };
            
            // Additional event for debugging
            recognition.onaudiostart = function() {
                debugLog('üîä Audio capture started', 'success');
            };
            
            recognition.onaudioend = function() {
                debugLog('üîä Audio capture ended', 'warning');
            };
            
            recognition.onspeechstart = function() {
                debugLog('üó£Ô∏è Speech detected!', 'success');
            };
            
            recognition.onspeechend = function() {
                debugLog('üó£Ô∏è Speech ended', 'warning');
            };
            
            debugLog('‚úÖ Speech recognition fully configured', 'success');
            return true;
            
        } catch (error) {
            debugLog(`‚ùå Error initializing speech: ${error.message}`, 'error');
            debugLog(`Error stack: ${error.stack}`, 'error');
            return false;
        }
    }
    
    // ENHANCED: Start microphone with comprehensive debugging
    function startMicrophone() {
        debugLog('STEP 3: Starting microphone...', 'info');
        
        if (!recognition) {
            debugLog('‚ùå Cannot start microphone - recognition not initialized', 'error');
            return false;
        }
        
        if (isListening) {
            debugLog('‚ö†Ô∏è Microphone already listening', 'warning');
            return true;
        }
        
        if (interviewCompleted) {
            debugLog('‚ö†Ô∏è Cannot start microphone - interview completed', 'warning');
            return false;
        }
        
        // Check if we have audio tracks
        if (userStream) {
            const audioTracks = userStream.getAudioTracks();
            debugLog(`Audio tracks available: ${audioTracks.length}`, 'info');
            if (audioTracks.length > 0) {
                debugLog(`Primary audio track enabled: ${audioTracks[0].enabled}`, 'info');
            }
        } else {
            debugLog('‚ö†Ô∏è No userStream available when starting microphone', 'warning');
        }
        
        try {
            isListening = true;
            debugLog('üé§ Calling recognition.start()...', 'info');
            recognition.start();
            debugLog('üé§ recognition.start() called successfully', 'success');
            return true;
        } catch (e) {
            debugLog(`‚ùå Failed to start microphone: ${e.name} - ${e.message}`, 'error');
            
            if (e.name === 'InvalidStateError') {
                debugLog('‚ö†Ô∏è Recognition already running, setting listening flag', 'warning');
                isListening = true;
                return true;
            } else if (e.name === 'NotAllowedError') {
                debugLog('‚ùå CRITICAL: Microphone permission denied', 'error');
                document.getElementById('conversationArea').innerHTML = 
                    `<div style="color: #f44336; padding: 20px; text-align: center;">
                        <h3>Microphone Permission Required</h3>
                        <p>Please allow microphone access and refresh the page.</p>
                        <button onclick="location.reload()" style="background: #1a73e8; color: white; border: none; padding: 10px 20px; border-radius: 5px; cursor: pointer; margin-top: 15px;">
                            Refresh & Grant Access
                        </button>
                    </div>`;
                isListening = false;
                return false;
            } else {
                isListening = false;
                debugLog('‚ùå Will retry microphone in 2 seconds...', 'warning');
                setTimeout(() => {
                    startMicrophone();
                }, 2000);
                return false;
            }
        }
    }
    
    // Auto-submit response
    function autoSubmitResponse() {
        if (isProcessingResponse || isInterviewerSpeaking || !collectedText.trim() || interviewCompleted) {
            return;
        }
        
        debugLog(`Submitting response: "${collectedText.substring(0, 50)}..."`, 'info');
        isProcessingResponse = true;
        
        if (speechTimeout) {
            clearTimeout(speechTimeout);
            speechTimeout = null;
        }
        
        const responseText = collectedText.trim();
        collectedText = '';
        
        // Show processing state
        document.getElementById('conversationArea').innerHTML = 
            `<div style="opacity: 0.7; font-style: italic;">Processing your response...</div>`;
        
        sendResponse(responseText).finally(() => {
            isProcessingResponse = false;
        });
    }
    
    // Send response to server
    async function sendResponse(text) {
        try {
            debugLog('Sending response to server', 'info');
            
            const response = await fetch(window.location.href, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/x-www-form-urlencoded',
                    'X-CSRFToken': TEMPLATE_DATA.csrfToken,
                },
                body: new URLSearchParams({ text: text })
            });
            
            if (!response.ok) {
                throw new Error(`Server responded with ${response.status}`);
            }
            
            const data = await response.json();
            debugLog('Server response received', 'success');
            
            if (data.success) {
                questionCount = data.question_count || questionCount + 1;
                document.getElementById('questionCount').textContent = `${questionCount}/8`;
                
                setTimeout(() => {
                    showCurrentQuestion(data.response, data.audio, data.audio_duration);
                }, 500);
                
                if (data.is_final || questionCount >= 8) {
                    debugLog('Interview completed', 'success');
                    interviewCompleted = true;
                }
            } else {
                debugLog('Server returned error', 'error');
                document.getElementById('conversationArea').innerHTML = 
                    `<div style="color: #f44336; padding: 20px;">There was an issue processing your response. Please try again.</div>`;
            }
        } catch (error) {
            debugLog(`Network error: ${error.message}`, 'error');
            document.getElementById('conversationArea').innerHTML = 
                `<div style="color: #f44336; padding: 20px;">Network error. Please check your connection and try again.</div>`;
        }
    }
    
    // Show current question with audio handling
    function showCurrentQuestion(question, audioUrl = null, audioDuration = null) {
        debugLog(`Showing question: ${question.substring(0, 50)}...`, 'info');
        
        const element = document.getElementById('conversationArea');
        const audioStatus = document.getElementById('audioStatus');
        const micBtn = document.getElementById('micBtn');
        const aiVideoBox = document.getElementById('aiVideoBox');
        
        element.innerHTML = '';
        isProcessingResponse = true;
        
        if (audioEndedTimeout) {
            clearTimeout(audioEndedTimeout);
            audioEndedTimeout = null;
        }
        
        // Check if we have valid audio
        const hasValidAudio = audioUrl && 
                              audioUrl.trim() !== '' && 
                              audioUrl !== 'None' && 
                              audioUrl !== 'null' &&
                              audioDuration && 
                              audioDuration > 0;
        
        if (hasValidAudio) {
            debugLog('Starting audio playback', 'info');
            
            isInterviewerSpeaking = true;
            micBtn.classList.add('disabled');
            aiVideoBox.classList.add('ai-speaking');
            audioStatus.className = 'audio-status playing';
            audioStatus.textContent = 'Interviewer speaking - Please listen';
            audioStatus.style.display = 'block';
            
            const audio = document.getElementById('aiAudio');
            currentAudioElement = audio;
            
            // Setup audio
            audio.pause();
            audio.currentTime = 0;
            audio.src = audioUrl.startsWith('/media/') ? audioUrl : '/media/' + audioUrl.replace(/^\/+/, '');
            audio.volume = 0.8;
            
            const onCanPlayThrough = () => {
                audio.removeEventListener('canplaythrough', onCanPlayThrough);
                
                audio.play().then(() => {
                    debugLog('Audio playing successfully', 'success');
                    // Start typewriter with audio sync
                    if (window.TypewriterSync) {
                        window.TypewriterSync.start(element, question, audio, audioDuration);
                    } else {
                        startFallbackTypewriter(element, question, audioDuration);
                    }
                }).catch(error => {
                    debugLog(`Audio play failed: ${error.message}`, 'error');
                    enableCandidateResponse();
                    startFallbackTypewriter(element, question);
                });
            };
            
            const onAudioEnded = () => {
                debugLog('Audio playback ended', 'success');
                audio.removeEventListener('ended', onAudioEnded);
                enableCandidateResponse();
            };
            
            const onAudioError = (error) => {
                debugLog(`Audio error: ${error}`, 'error');
                audio.removeEventListener('error', onAudioError);
                enableCandidateResponse();
                startFallbackTypewriter(element, question);
            };
            
            // Attach event listeners
            audio.addEventListener('canplaythrough', onCanPlayThrough);
            audio.addEventListener('ended', onAudioEnded);
            audio.addEventListener('error', onAudioError);
            
            // Timeout fallback
            audioEndedTimeout = setTimeout(() => {
                if (isInterviewerSpeaking) {
                    debugLog('Audio timeout - enabling microphone', 'warning');
                    enableCandidateResponse();
                }
            }, (audioDuration + 5) * 1000);
            
            try {
                audio.load();
            } catch (error) {
                debugLog(`Audio load failed: ${error.message}`, 'error');
                enableCandidateResponse();
                startFallbackTypewriter(element, question);
            }
            
        } else {
            debugLog('No valid audio, using text-only typewriter', 'info');
            if (window.TypewriterSync) {
                window.TypewriterSync.createTextOnly(element, question, { fast: false }).then(() => {
                    setTimeout(() => enableCandidateResponse(), 800);
                });
            } else {
                startFallbackTypewriter(element, question);
            }
        }
    }
    
    function startFallbackTypewriter(element, question, audioDuration = null) {
        debugLog('Using fallback typewriter', 'info');
        let index = 0;
        const targetDuration = audioDuration ? audioDuration * 1000 : Math.min(question.length * 60, 6000);
        const charDelay = Math.max(30, Math.min(100, targetDuration / question.length));
        
        const typeChar = () => {
            if (index < question.length) {
                element.textContent += question.charAt(index);
                index++;
                setTimeout(typeChar, charDelay);
            } else {
                setTimeout(() => enableCandidateResponse(), 800);
            }
        };
        typeChar();
    }
    
    function enableCandidateResponse() {
        debugLog('Enabling candidate response', 'success');
        
        isInterviewerSpeaking = false;
        isProcessingResponse = false;
        
        const micBtn = document.getElementById('micBtn');
        const audioStatus = document.getElementById('audioStatus');
        const aiVideoBox = document.getElementById('aiVideoBox');
        
        micBtn.classList.remove('disabled');
        aiVideoBox.classList.remove('ai-speaking');
        audioStatus.className = 'audio-status listening';
        audioStatus.textContent = 'Your turn - Speak your answer';
        
        setTimeout(() => {
            audioStatus.style.display = 'none';
        }, 4000);
        
        if (audioEndedTimeout) {
            clearTimeout(audioEndedTimeout);
            audioEndedTimeout = null;
        }
    }
    
    // Toggle microphone
    function toggleMic() {
        if (!recognition || interviewCompleted) {
            return;
        }
        
        if (isInterviewerSpeaking) {
            alert('Please wait for the interviewer to finish speaking.');
            return;
        }
        
        if (isListening) {
            debugLog('Turning off microphone', 'warning');
            isListening = false;
            try {
                recognition.stop();
            } catch (e) {
                debugLog(`Error stopping recognition: ${e.message}`, 'error');
            }
            document.getElementById('micBtn').classList.remove('recording');
            
            if (collectedText.trim()) {
                autoSubmitResponse();
            }
        } else {
            debugLog('Turning on microphone', 'success');
            isListening = true;
            collectedText = '';
            try {
                recognition.start();
            } catch (e) {
                debugLog(`Error starting recognition: ${e.message}`, 'error');
                isListening = false;
            }
        }
    }
    
    // Toggle camera
    function toggleCamera() {
        if (!userStream) return;
        
        const videoTracks = userStream.getVideoTracks();
        isCameraOn = !isCameraOn;
        
        videoTracks.forEach(track => {
            track.enabled = isCameraOn;
        });
        
        const btn = document.getElementById('cameraBtn');
        btn.style.background = isCameraOn ? '#34a853' : '#666';
        btn.title = isCameraOn ? 'Camera ON' : 'Camera OFF';
        
        debugLog(`Camera ${isCameraOn ? 'enabled' : 'disabled'}`, 'info');
    }
    
    // End interview
    function endInterview() {
        if (confirm('Are you sure you want to end the interview?')) {
            debugLog('Interview ended by user', 'warning');
            interviewCompleted = true;
            
            // Stop all systems
            if (recognition) {
                try {
                    recognition.stop();
                } catch (e) {}
            }
            
            if (userStream) {
                userStream.getTracks().forEach(track => track.stop());
            }
            
            document.getElementById('conversationArea').innerHTML = 
                `<div style="text-align: center; padding: 40px;">
                    <h2>Interview Ended</h2>
                    <p>Thank you for your time. You may now close this window.</p>
                </div>`;
            
            // Redirect after a delay
            setTimeout(() => {
                window.location.href = '/';
            }, 3000);
        }
    }
    
    // Timer
    function updateTimer() {
        if (interviewCompleted) return;
        
        const minutes = Math.floor(timeLeft / 60);
        const seconds = timeLeft % 60;
        document.getElementById('timer').textContent = 
            `${minutes}:${seconds.toString().padStart(2, '0')}`;
        
        if (timeLeft <= 0) {
            debugLog('Time expired', 'warning');
            return;
        }
        
        timeLeft--;
    }
    
    // Initialize everything
    document.addEventListener('DOMContentLoaded', function() {
        debugLog('DOM loaded, initializing systems...', 'info');
        
        // Initialize camera first
        initCamera().then(() => {
            debugLog('Camera initialization complete', 'success');
            
            // Then initialize speech with delay
            setTimeout(() => {
                debugLog('Starting speech recognition initialization...', 'info');
                initSpeech();
            }, 2000);
        });
        
        // Start timer
        setInterval(() => {
            updateTimer();
        }, 1000);
        
        // Show initial question with proper validation
        setTimeout(() => {
            debugLog('Showing initial question...', 'info');
            
            const question = TEMPLATE_DATA.initialQuestion || "Welcome to your AI interview! Let's begin with your introduction.";
            const audioUrl = TEMPLATE_DATA.initialAudio || null;
            const duration = TEMPLATE_DATA.initialDuration || 5;
            
            if (question.length < 10) {
                debugLog('Question too short, using fallback', 'warning');
                const fallbackQuestion = `Hello ${TEMPLATE_DATA.candidateName}! Welcome to your AI interview. Could you please start by telling me about yourself and your background?`;
                showCurrentQuestion(fallbackQuestion, audioUrl, duration);
            } else {
                showCurrentQuestion(question, audioUrl, duration);
            }
        }, 3000);
        
        debugLog('System initialization complete', 'success');
    });
    
    // Error handling
    window.addEventListener('error', function(e) {
        debugLog(`JavaScript error: ${e.message}`, 'error');
        console.error('Global error:', e);
    });
    
    window.addEventListener('unhandledrejection', function(e) {
        debugLog(`Unhandled promise rejection: ${e.reason}`, 'error');
        console.error('Unhandled rejection:', e);
    });
    
    </script>
</body>
</html>