{% load static %}
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Interview - {{ candidate_name }}</title>
    <link rel="stylesheet" href="{% static 'css/professional-interview.css' %}">
    <style>
        body {
            background: #202124;
            color: white;
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 20px;
            height: 100vh;
            overflow: hidden;
        }
        .container {
            max-width: 1400px;
            margin: 0 auto;
            height: calc(100vh - 40px);
            display: flex;
            flex-direction: column;
        }
        .video-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin-bottom: 20px;
        }
        .video-box {
            background: #303134;
            border-radius: 12px;
            height: 300px;
            display: flex;
            align-items: center;
            justify-content: center;
            position: relative;
            border: 2px solid #444;
        }
        .ai-avatar {
            font-size: 80px;
            animation: pulse 2s ease-in-out infinite alternate;
        }
        .ai-speaking {
            animation: pulse 1s ease-in-out infinite alternate;
            border-color: #ff9800 !important;
            box-shadow: 0 0 20px rgba(255, 152, 0, 0.3);
        }
        .candidate-speaking {
            border-color: #4caf50 !important;
            box-shadow: 0 0 20px rgba(76, 175, 80, 0.3);
        }
        video {
            width: 100%;
            height: 100%;
            object-fit: cover;
            border-radius: 12px;
        }
        .participant-name {
            position: absolute;
            bottom: 16px;
            left: 16px;
            background: rgba(0,0,0,0.8);
            padding: 8px 16px;
            border-radius: 20px;
            font-size: 16px;
            font-weight: bold;
            backdrop-filter: blur(10px);
        }
        .controls {
            position: fixed;
            bottom: 20px;
            left: 50%;
            transform: translateX(-50%);
            text-align: center;
            z-index: 1000;
            display: flex;
            gap: 15px;
        }
        .btn {
            background: #34a853;
            color: white;
            border: none;
            padding: 15px 20px;
            border-radius: 50px;
            font-size: 24px;
            cursor: pointer;
            transition: all 0.3s ease;
            min-width: 60px;
            height: 60px;
            display: flex;
            align-items: center;
            justify-content: center;
        }
        .btn:hover {
            transform: scale(1.05);
            box-shadow: 0 4px 15px rgba(0,0,0,0.3);
        }
        .btn.recording {
            background: #ea4335;
            animation: recording-pulse 1s infinite;
        }
        .btn.disabled {
            background: #666;
            cursor: not-allowed;
            opacity: 0.5;
            pointer-events: none;
        }
        .transcript-section {
            background: #303134;
            padding: 25px;
            border-radius: 12px;
            margin: 20px 0;
            flex: 1;
            display: flex;
            flex-direction: column;
            border: 2px solid #444;
            min-height: 300px;
        }
        .conversation-text {
            background: transparent;
            padding: 20px;
            border-radius: 8px;
            color: white;
            font-size: 18px;
            line-height: 1.6;
            text-align: center;
            flex: 1;
            overflow-y: auto;
        }
        .debug-panel {
            background: #1a1a1a;
            padding: 15px;
            border-radius: 8px;
            margin-top: 10px;
            font-size: 12px;
            font-family: 'Courier New', monospace;
            color: #90a4ae;
            max-height: 150px;
            overflow-y: auto;
        }
        .debug-panel .status {
            margin: 5px 0;
            padding: 3px 8px;
            border-radius: 4px;
        }
        .debug-panel .status.success { background: rgba(76, 175, 80, 0.2); color: #81c784; }
        .debug-panel .status.warning { background: rgba(255, 152, 0, 0.2); color: #ffb74d; }
        .debug-panel .status.error { background: rgba(244, 67, 54, 0.2); color: #e57373; }
        .debug-panel .status.info { background: rgba(33, 150, 243, 0.2); color: #64b5f6; }
        
        .stats {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 20px;
            padding: 0 10px;
        }
        .stat {
            background: #303134;
            padding: 12px 24px;
            border-radius: 25px;
            font-weight: 500;
            border: 2px solid #444;
        }
        .audio-status {
            position: fixed;
            top: 20px;
            left: 50%;
            transform: translateX(-50%);
            background: #1a73e8;
            color: white;
            padding: 12px 20px;
            border-radius: 25px;
            font-size: 14px;
            font-weight: 500;
            z-index: 1000;
            display: none;
            box-shadow: 0 4px 15px rgba(0,0,0,0.3);
        }
        .audio-status.playing {
            display: block;
            background: #ff9800;
            animation: status-pulse 2s ease-in-out infinite;
        }
        .audio-status.listening {
            display: block;
            background: #4caf50;
            animation: status-pulse 2s ease-in-out infinite;
        }
        
        /* Animations */
        @keyframes pulse {
            0%, 100% { opacity: 0.8; }
            50% { opacity: 1; }
        }
        @keyframes recording-pulse {
            0%, 100% { transform: scale(1); box-shadow: 0 0 0 0 rgba(234, 67, 53, 0.7); }
            50% { transform: scale(1.05); box-shadow: 0 0 0 10px rgba(234, 67, 53, 0); }
        }
        @keyframes status-pulse {
            0%, 100% { opacity: 0.9; }
            50% { opacity: 1; }
        }
    </style>
</head>
<body>
    <div class="container">
        <!-- Audio status indicator -->
        <div class="audio-status" id="audioStatus"></div>
        
        <div class="stats">
            <div class="stat">Question: <span id="questionCount">1/8</span></div>
            <div class="stat">Time: <span id="timer">15:00</span></div>
            {% comment %} <div class="stat">Debug Mode</div> {% endcomment %}
        </div>
        
        <div class="video-grid">
            <div class="video-box" id="aiVideoBox">
                <div class="ai-avatar" id="aiAvatar">ü§ñ</div>
                <div class="participant-name">Alex - AI Interviewer</div>
            </div>
            <div class="video-box" id="candidateVideoBox">
                <video id="userVideo" autoplay muted playsinline></video>
                <div class="participant-name">{{ candidate_name|default:"Candidate" }}</div>
            </div>
        </div>

        <div class="transcript-section">
            <div class="conversation-text" id="conversationArea">
                
            </div>
            
            {% comment %} <!-- DEBUG PANEL -->
            <div class="debug-panel" id="debugPanel">
                <strong>Debug Log:</strong><br>
                <div id="debugLog">System starting...</div>
            </div>
        </div> {% endcomment %}

        <div class="controls">
            <button class="btn recording" id="micBtn" onclick="toggleMic()" title="Microphone">üé§</button>
            <button class="btn" id="cameraBtn" onclick="toggleCamera()" title="Camera">üìπ</button>
            <button class="btn" id="testSpeechBtn" onclick="testSpeechRecognition()" title="Test Speech" style="background: #2196f3;">üîä</button>
        </div>
    </div>

    <!-- Audio element -->
    <audio id="aiAudio" preload="auto"></audio>
    {% csrf_token %}
    
    <script src="{% static 'js/typewriter-sync.js' %}"></script>

    <script>
    // Global variables
    let recognition = null;
    let isListening = false;
    let questionCount = 1;
    let timeLeft = 15 * 60;
    let userStream = null;
    let isCameraOn = true;
    let collectedText = '';
    let speechTimeout = null;
    let isProcessingResponse = false;
    let isInterviewerSpeaking = false;
    let interviewCompleted = false;
    let audioEndedTimeout = null;
    let currentAudioElement = null;
    
    // DEBUG: Enhanced logging
    function debugLog(message, type = 'info') {
        const timestamp = new Date().toLocaleTimeString();
        const logElement = document.getElementById('debugLog');
        const statusClass = type;
        
        console.log(`[${timestamp}] ${message}`);
        
        const logEntry = document.createElement('div');
        logEntry.className = `status ${statusClass}`;
        logEntry.innerHTML = `<span style="color: #666;">${timestamp}</span> ${message}`;
        
        logElement.appendChild(logEntry);
        logElement.scrollTop = logElement.scrollHeight;
        
        // Keep only last 20 entries
        while (logElement.children.length > 20) {
            logElement.removeChild(logElement.firstChild);
        }
    }
    
    debugLog('AI Interview System Starting...', 'info');
    
    // Test speech recognition availability
    function testSpeechRecognition() {
        debugLog('Testing speech recognition...', 'info');
        
        if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
            debugLog('‚ùå Speech recognition NOT supported in this browser', 'error');
            alert('Speech recognition not supported. Please use Chrome, Edge, or Safari.');
            return;
        }
        
        debugLog('‚úÖ Speech recognition API available', 'success');
        
        try {
            const SpeechRecognition = window.webkitSpeechRecognition || window.SpeechRecognition;
            const testRecognition = new SpeechRecognition();
            
            debugLog('‚úÖ SpeechRecognition object created successfully', 'success');
            
            testRecognition.continuous = false;
            testRecognition.interimResults = true;
            testRecognition.lang = 'en-US';
            
            testRecognition.onstart = function() {
                debugLog('‚úÖ Test recognition started', 'success');
                setTimeout(() => {
                    testRecognition.stop();
                }, 3000);
            };
            
            testRecognition.onresult = function(event) {
                const result = event.results[0][0].transcript;
                debugLog(`‚úÖ Test speech captured: "${result}"`, 'success');
            };
            
            testRecognition.onerror = function(event) {
                debugLog(`‚ùå Test recognition error: ${event.error}`, 'error');
            };
            
            testRecognition.onend = function() {
                debugLog('‚úÖ Test recognition ended', 'info');
            };
            
            testRecognition.start();
            debugLog('Starting 3-second speech test...', 'info');
            
        } catch (error) {
            debugLog(`‚ùå Error creating test recognition: ${error.message}`, 'error');
        }
    }
    
    // Initialize camera and microphone with enhanced debugging
    async function initCamera() {
        try {
            debugLog('Requesting camera and microphone permissions...', 'info');
            
            userStream = await navigator.mediaDevices.getUserMedia({ 
                video: {
                    width: { ideal: 1280 },
                    height: { ideal: 720 },
                    facingMode: 'user'
                }, 
                audio: {
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true,
                    sampleRate: 44100
                }
            });
            
            const videoElement = document.getElementById('userVideo');
            videoElement.srcObject = userStream;
            
            // Check audio tracks
            const audioTracks = userStream.getAudioTracks();
            const videoTracks = userStream.getVideoTracks();
            
            debugLog(`‚úÖ Camera access granted - ${videoTracks.length} video tracks`, 'success');
            debugLog(`‚úÖ Microphone access granted - ${audioTracks.length} audio tracks`, 'success');
            
            if (audioTracks.length > 0) {
                debugLog(`Microphone: ${audioTracks[0].label}`, 'info');
                debugLog(`Microphone enabled: ${audioTracks[0].enabled}`, 'info');
            }
            
        } catch (error) {
            debugLog(`‚ùå Camera/Microphone error: ${error.name} - ${error.message}`, 'error');
            
            let message = 'Camera/Microphone Error: ';
            if (error.name === 'NotAllowedError') {
                message += 'Please allow camera and microphone access, then refresh this page.';
            } else if (error.name === 'NotFoundError') {
                message += 'No camera or microphone found. Please connect devices and refresh.';
            } else {
                message += 'Cannot access camera/microphone. Please check permissions and refresh.';
            }
            
            document.getElementById('conversationArea').innerHTML = 
                `<div style="color: #f44336; padding: 20px;">${message}</div>`;
        }
    }
    
    // Enhanced speech recognition initialization
    function initSpeech() {
        debugLog('Initializing speech recognition...', 'info');
        
        if (!('webkitSpeechRecognition' in window) && !('SpeechRecognition' in window)) {
            debugLog('‚ùå Speech recognition not supported', 'error');
            document.getElementById('conversationArea').innerHTML = 
                `<div style="color: #f44336; padding: 20px;">Speech recognition not supported. Please use Chrome, Edge, or Safari.</div>`;
            return;
        }
        
        try {
            const SpeechRecognition = window.webkitSpeechRecognition || window.SpeechRecognition;
            recognition = new SpeechRecognition();
            
            debugLog('‚úÖ SpeechRecognition object created', 'success');
            
            // Configure speech recognition
            recognition.continuous = true;
            recognition.interimResults = true;
            recognition.lang = 'en-US';
            recognition.maxAlternatives = 1;
            
            debugLog('Speech recognition configured', 'info');
            
            recognition.onstart = function() {
                isListening = true;
                document.getElementById('micBtn').classList.add('recording');
                document.getElementById('candidateVideoBox').classList.add('candidate-speaking');
                debugLog('üéôÔ∏è Speech recognition started', 'success');
            };
            
            recognition.onend = function() {
                document.getElementById('candidateVideoBox').classList.remove('candidate-speaking');
                debugLog('üéôÔ∏è Speech recognition ended', 'warning');
                
                // Auto-restart if still supposed to be listening
                if (isListening && !interviewCompleted && !isInterviewerSpeaking) {
                    setTimeout(() => {
                        try {
                            if (recognition && isListening) {
                                debugLog('üîÑ Restarting speech recognition', 'info');
                                recognition.start();
                            }
                        } catch (e) {
                            debugLog(`‚ö†Ô∏è Could not restart recognition: ${e.message}`, 'warning');
                        }
                    }, 100);
                }
            };
            
            recognition.onerror = function(event) {
                debugLog(`‚ùå Speech recognition error: ${event.error}`, 'error');
                document.getElementById('candidateVideoBox').classList.remove('candidate-speaking');
                
                switch(event.error) {
                    case 'not-allowed':
                        debugLog('‚ùå Microphone access denied', 'error');
                        break;
                    case 'no-speech':
                        debugLog('‚ö†Ô∏è No speech detected', 'warning');
                        break;
                    case 'audio-capture':
                        debugLog('‚ùå Microphone not available', 'error');
                        break;
                    case 'network':
                        debugLog('üåê Network error', 'warning');
                        break;
                }
            };
            
            recognition.onresult = function(event) {
                let interimTranscript = '';
                let finalTranscript = '';
                
                for (let i = event.resultIndex; i < event.results.length; i++) {
                    const transcript = event.results[i][0].transcript;
                    if (event.results[i].isFinal) {
                        finalTranscript += transcript;
                    } else {
                        interimTranscript += transcript;
                    }
                }
                
                debugLog(`üë§ Speech result: "${finalTranscript || interimTranscript}"`, 'success');
                
                // Only process speech when candidate can speak
                if (!isInterviewerSpeaking && !isProcessingResponse && !interviewCompleted) {
                    
                    // Show the speech in real-time
                    const displayText = collectedText + finalTranscript + 
                        (interimTranscript ? `<span style="opacity: 0.7; font-style: italic; color: #90a4ae;">${interimTranscript}</span>` : '');
                    
                    if (displayText.trim()) {
                        document.getElementById('conversationArea').innerHTML = 
                            `<div style="color: #81c784; font-style: italic;">${displayText}</div>`;
                    }
                
                    if (finalTranscript.trim()) {
                        collectedText += finalTranscript;
                        debugLog(`üìù Collected text length: ${collectedText.length}`, 'info');
                    
                        // Clear existing timeout
                        if (speechTimeout) {
                            clearTimeout(speechTimeout);
                        }
                    
                        // Auto-submit after speech pause
                        speechTimeout = setTimeout(() => {
                            if (collectedText.trim() && !isProcessingResponse && !isInterviewerSpeaking && !interviewCompleted) {
                                debugLog('‚ö° Auto-submitting response', 'info');
                                autoSubmitResponse();
                            }
                        }, 2500);
                    }
                } else {
                    if (isInterviewerSpeaking) {
                        debugLog('üö´ Ignoring speech - interviewer speaking', 'warning');
                    } else if (isProcessingResponse) {
                        debugLog('üö´ Ignoring speech - processing response', 'warning');
                    }
                }
            };
            
            startMicrophone();
            
        } catch (error) {
            debugLog(`‚ùå Error initializing speech: ${error.message}`, 'error');
        }
    }
    
    // Start microphone with debugging
    function startMicrophone() {
        if (recognition && !isListening && !interviewCompleted) {
            try {
                isListening = true;
                recognition.start();
                debugLog('üéôÔ∏è Microphone started successfully', 'success');
            } catch (e) {
                debugLog(`‚ùå Failed to start microphone: ${e.message}`, 'error');
                if (e.name === 'InvalidStateError') {
                    debugLog('‚ö†Ô∏è Recognition already running', 'warning');
                    isListening = true;
                } else {
                    isListening = false;
                }
            }
        } else {
            debugLog('‚ö†Ô∏è Cannot start microphone - conditions not met', 'warning');
        }
    }
    
    // Auto-submit response
    function autoSubmitResponse() {
        if (isProcessingResponse || isInterviewerSpeaking || !collectedText.trim() || interviewCompleted) {
            debugLog('‚ö†Ô∏è Cannot submit - conditions not met', 'warning');
            return;
        }
        
        debugLog(`üì§ Submitting response: "${collectedText.substring(0, 100)}..."`, 'info');
        isProcessingResponse = true;
        
        if (speechTimeout) {
            clearTimeout(speechTimeout);
            speechTimeout = null;
        }
        
        const responseText = collectedText.trim();
        collectedText = '';
        
        // Show processing state
        document.getElementById('conversationArea').innerHTML = 
            `<div style="opacity: 0.7; font-style: italic;">Processing your response...</div>`;
        
        sendResponse(responseText).finally(() => {
            isProcessingResponse = false;
        });
    }
    
    // Send response to server
    async function sendResponse(text) {
        try {
            debugLog('üì° Sending response to server', 'info');
            
            const response = await fetch(window.location.href, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/x-www-form-urlencoded',
                    'X-CSRFToken': document.querySelector('[name=csrfmiddlewaretoken]').value,
                },
                body: new URLSearchParams({ text: text })
            });
            
            if (!response.ok) {
                throw new Error(`Server responded with ${response.status}`);
            }
            
            const data = await response.json();
            debugLog('‚úÖ Server response received', 'success');
            
            if (data.success) {
                questionCount = data.question_count || questionCount + 1;
                document.getElementById('questionCount').textContent = `${questionCount}/8`;
                
                setTimeout(() => {
                    showCurrentQuestion(data.response, data.audio, data.audio_duration);
                }, 500);
                
                if (data.is_final || questionCount >= 8) {
                    debugLog('üèÅ Interview completed', 'success');
                }
            } else {
                debugLog('‚ùå Server returned error', 'error');
            }
        } catch (error) {
            debugLog(`‚ùå Network error: ${error.message}`, 'error');
        }
    }
    
    // Show current question with debugging
    function showCurrentQuestion(question, audioUrl = null, audioDuration = null) {
        debugLog(`üìù Showing question: ${question.substring(0, 50)}...`, 'info');
        debugLog(`üîä Audio URL: ${audioUrl || 'none'}`, 'info');
        debugLog(`‚è±Ô∏è Duration: ${audioDuration || 'none'}s`, 'info');
        
        const element = document.getElementById('conversationArea');
        const audioStatus = document.getElementById('audioStatus');
        const micBtn = document.getElementById('micBtn');
        const aiVideoBox = document.getElementById('aiVideoBox');
        
        element.innerHTML = '';
        isProcessingResponse = true;
        
        if (audioEndedTimeout) {
            clearTimeout(audioEndedTimeout);
            audioEndedTimeout = null;
        }
        
        const hasValidAudio = audioUrl && audioUrl.trim() !== '' && audioUrl !== 'None' && audioDuration > 0;
        
        if (hasValidAudio) {
            debugLog('üîä Starting audio playback', 'info');
            
            isInterviewerSpeaking = true;
            micBtn.classList.add('disabled');
            aiVideoBox.classList.add('ai-speaking');
            audioStatus.className = 'audio-status playing';
            audioStatus.textContent = 'Interviewer speaking - Please listen';
            audioStatus.style.display = 'block';
            
            const audio = document.getElementById('aiAudio');
            currentAudioElement = audio;
            audio.pause();
            audio.currentTime = 0;
            audio.src = audioUrl;
            audio.volume = 0.8;
            
            const onCanPlayThrough = () => {
                debugLog('üîä Audio ready, starting playback', 'success');
                audio.removeEventListener('canplaythrough', onCanPlayThrough);
                
                audio.play().then(() => {
                    debugLog('üîä Audio playing', 'success');
                    if (window.TypewriterSync) {
                        window.TypewriterSync.start(element, question, audio, audioDuration);
                    } else {
                        startFallbackTypewriter(element, question, audioDuration);
                    }
                }).catch(error => {
                    debugLog(`‚ùå Audio play failed: ${error.message}`, 'error');
                    enableCandidateResponse();
                    startFallbackTypewriter(element, question);
                });
            };
            
            const onAudioEnded = () => {
                debugLog('‚úÖ Audio playback ended', 'success');
                audio.removeEventListener('ended', onAudioEnded);
                enableCandidateResponse();
            };
            
            const onAudioError = (error) => {
                debugLog(`‚ùå Audio error: ${error.message}`, 'error');
                audio.removeEventListener('error', onAudioError);
                enableCandidateResponse();
                startFallbackTypewriter(element, question);
            };
            
            audio.addEventListener('canplaythrough', onCanPlayThrough);
            audio.addEventListener('ended', onAudioEnded);
            audio.addEventListener('error', onAudioError);
            
            audioEndedTimeout = setTimeout(() => {
                if (isInterviewerSpeaking) {
                    debugLog('‚è∞ Audio timeout - enabling microphone', 'warning');
                    enableCandidateResponse();
                }
            }, (audioDuration + 5) * 1000);
            
            try {
                audio.load();
            } catch (error) {
                debugLog(`‚ùå Audio load failed: ${error.message}`, 'error');
                enableCandidateResponse();
                startFallbackTypewriter(element, question);
            }
            
        } else {
            debugLog('üìù No audio, using text-only typewriter', 'info');
            if (window.TypewriterSync) {
                window.TypewriterSync.createTextOnly(element, question, { fast: false }).then(() => {
                    setTimeout(() => enableCandidateResponse(), 800);
                });
            } else {
                startFallbackTypewriter(element, question);
            }
        }
    }
    
    function startFallbackTypewriter(element, question, audioDuration = null) {
        debugLog('üìù Using fallback typewriter', 'info');
        let index = 0;
        const targetDuration = audioDuration ? audioDuration * 1000 : Math.min(question.length * 60, 6000);
        const charDelay = Math.max(30, Math.min(100, targetDuration / question.length));
        
        const typeChar = () => {
            if (index < question.length) {
                element.textContent += question.charAt(index);
                index++;
                setTimeout(typeChar, charDelay);
            } else {
                setTimeout(() => enableCandidateResponse(), 800);
            }
        };
        typeChar();
    }
    
    function enableCandidateResponse() {
        debugLog('üé§ Enabling candidate response', 'success');
        
        isInterviewerSpeaking = false;
        isProcessingResponse = false;
        
        const micBtn = document.getElementById('micBtn');
        const audioStatus = document.getElementById('audioStatus');
        const aiVideoBox = document.getElementById('aiVideoBox');
        
        micBtn.classList.remove('disabled');
        aiVideoBox.classList.remove('ai-speaking');
        audioStatus.className = 'audio-status listening';
        audioStatus.textContent = 'Your turn - Speak your answer';
        
        setTimeout(() => {
            audioStatus.style.display = 'none';
        }, 4000);
        
        if (audioEndedTimeout) {
            clearTimeout(audioEndedTimeout);
            audioEndedTimeout = null;
        }
    }
    
    // Toggle microphone
    function toggleMic() {
        if (!recognition || interviewCompleted) {
            debugLog('‚ö†Ô∏è Cannot toggle mic - not available', 'warning');
            return;
        }
        
        if (isInterviewerSpeaking) {
            debugLog('‚ö†Ô∏è Cannot toggle mic - interviewer speaking', 'warning');
            alert('Please wait for the interviewer to finish speaking.');
            return;
        }
        
        if (isListening) {
            debugLog('üé§ Turning off microphone', 'warning');
            isListening = false;
            try {
                recognition.stop();
            } catch (e) {
                debugLog(`Error stopping recognition: ${e.message}`, 'error');
            }
            document.getElementById('micBtn').classList.remove('recording');
            
            if (collectedText.trim()) {
                autoSubmitResponse();
            }
        } else {
            debugLog('üé§ Turning on microphone', 'success');
            isListening = true;
            collectedText = '';
            try {
                recognition.start();
            } catch (e) {
                debugLog(`Error starting recognition: ${e.message}`, 'error');
                isListening = false;
            }
        }
    }
    
    // Toggle camera
    function toggleCamera() {
        if (!userStream) return;
        
        const videoTracks = userStream.getVideoTracks();
        isCameraOn = !isCameraOn;
        
        videoTracks.forEach(track => {
            track.enabled = isCameraOn;
        });
        
        const btn = document.getElementById('cameraBtn');
        if (isCameraOn) {
            btn.innerHTML = 'üìπ';
            btn.title = 'Camera is ON';
        } else {
            btn.innerHTML = 'üìπ';
            btn.title = 'Camera is OFF';
        }
        
        debugLog(`üìπ Camera ${isCameraOn ? 'enabled' : 'disabled'}`, 'info');
    }
    
    // Timer
    function updateTimer() {
        if (interviewCompleted) return;
        
        const minutes = Math.floor(timeLeft / 60);
        const seconds = timeLeft % 60;
        document.getElementById('timer').textContent = 
            `${minutes}:${seconds.toString().padStart(2, '0')}`;
        
        if (timeLeft <= 0) {
            debugLog('‚è∞ Time expired', 'warning');
            return;
        }
        
        timeLeft--;
    }
    
    // Initialize everything
    document.addEventListener('DOMContentLoaded', function() {
        debugLog('üöÄ DOM loaded, initializing systems...', 'info');
        
        // Initialize camera first
        initCamera().then(() => {
            debugLog('Camera initialization complete', 'success');
            
            // Then initialize speech with delay
            setTimeout(() => {
                debugLog('Starting speech recognition initialization...', 'info');
                initSpeech();
            }, 2000);
        });
        
        // Start timer
        setInterval(() => {
            updateTimer();
        }, 1000);
        
        // Get initial question data
        const initialQuestion = `{{ ai_question|escapejs|default:"Welcome to your AI interview! Let's begin." }}`;
        const initialAudio = `{{ audio_url|escapejs|safe }}`;
        const initialDuration = {{ audio_duration|default:"5" }};
        
        debugLog(`Initial question length: ${initialQuestion.length}`, 'info');
        debugLog(`Initial audio: ${initialAudio ? 'available' : 'none'}`, 'info');
        debugLog(`Initial duration: ${initialDuration}s`, 'info');
        
        // Show initial question after delay
        setTimeout(() => {
            debugLog('Showing initial question...', 'info');
            showCurrentQuestion(initialQuestion, initialAudio, initialDuration);
        }, 3000);
        
        debugLog('‚úÖ System initialization complete', 'success');
    });
</script>
</body>
</html>