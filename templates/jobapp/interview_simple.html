{% load static %}
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Interview</title>
    <link rel="stylesheet" href="{% static 'css/professional-interview.css' %}">
    <link rel="stylesheet" href="{% static 'css/interview-improvements.css' %}"
    <style>
        body {
            background: #202124;
            color: white;
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
            height: 100vh;
            overflow: hidden;
        }
        .container {
            max-width: 1400px;
            margin: 0 auto;
            height: calc(100vh - 40px);
            display: flex;
            flex-direction: column;
        }
        .video-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin-bottom: 20px;
        }
        .video-box {
            background: #303134;
            border-radius: 12px;
            height: 400px;
            display: flex;
            align-items: center;
            justify-content: center;
            position: relative;
        }
        .candidate-box {
            height: 400px;
        }
        .ai-avatar {
            font-size: 80px;
        }
        video {
            width: 100%;
            height: 100%;
            object-fit: cover;
            border-radius: 12px;
        }
        .participant-name {
            position: absolute;
            bottom: 16px;
            left: 16px;
            background: rgba(0,0,0,0.8);
            padding: 8px 16px;
            border-radius: 20px;
            font-size: 16px;
            font-weight: bold;
        }
        .controls {
            position: fixed;
            bottom: 20px;
            left: 50%;
            transform: translateX(-50%);
            text-align: center;
            z-index: 1000;
        }
        .btn {
            background: #34a853;
            color: white;
            border: none;
            padding: 15px 30px;
            border-radius: 25px;
            font-size: 16px;
            cursor: pointer;
            margin: 0 10px;
        }
        .btn:hover {
            opacity: 0.8;
        }
        .btn.recording {
            background: #ea4335;
            animation: pulse 1s infinite;
        }
        .btn.camera-off {
            background: #ea4335;
        }
        .transcript-section {
            background: #303134;
            padding: 20px;
            border-radius: 12px;
            margin: 20px 0;
            flex: 1;
            display: flex;
            align-items: center;
            justify-content: center;
            position: relative;
        }
        .conversation-text {
            background: transparent;
            padding: 20px;
            border-radius: 8px;
            color: white;
            font-size: 18px;
            line-height: 1.6;
            text-align: center;
            width: 100%;
        }
        .transcript-item {
            margin: 10px 0;
            padding: 10px;
            border-radius: 8px;
        }
        .interviewer-msg {
            background: #1a73e8;
            text-align: left;
        }
        .candidate-msg {
            background: #34a853;
            text-align: right;
        }
        .stats {
            display: flex;
            justify-content: space-between;
            margin-bottom: 20px;
        }
        .stat {
            background: #303134;
            padding: 10px 20px;
            border-radius: 20px;
        }
        @keyframes pulse {
            0%, 100% { transform: scale(1); }
            50% { transform: scale(1.05); }
        }
    </style>
</head>
<body>
    <div class="container">
        <!-- Speech Status Indicator -->
        <div class="speech-status candidate-ready" id="speechStatus">
            üé§ Ready for your response
        </div>
        
        <div class="stats">
            <div class="stat">Question: <span id="questionCount">1/8</span></div>
            <div class="stat">Time: <span id="timer">15:00</span></div>
        </div>
        
        <!-- Professional Interview Status Indicator -->
        <div class="interview-status recording" id="interviewStatus">
            <div class="status-dot"></div>
            
        </div>
        
        <!-- Audio Level Indicator -->
        <div class="audio-level-indicator" id="audioLevelIndicator">
            <div class="audio-level-bar"></div>
            <div class="audio-level-bar"></div>
            <div class="audio-level-bar"></div>
            <div class="audio-level-bar"></div>
            <div class="audio-level-bar"></div>
        </div>
        
        <div class="video-grid">
            <div class="video-box">
                <div class="ai-avatar">ü§ñ</div>
                <div class="participant-name">Alex - AI Interviewer</div>
            </div>
            <div class="video-box candidate-box recording">
                <video id="userVideo" autoplay muted playsinline></video>
                <div class="participant-name">{{ candidate_name|default:"Candidate" }}</div>
            </div>
        </div>
        
        <div class="transcript-section">
            <div class="conversation-text" id="conversationArea">
            </div>
        </div>
        
        <div class="controls">
            <button class="btn recording" id="micBtn" onclick="toggleMic()" title="Microphone is ON (Professional Interview Mode)">üî¥ </button>
            <button class="btn" id="cameraBtn" onclick="toggleCamera()">üìπ </button>
            <button class="btn" id="endBtn" onclick="endInterview()" style="background: #ea4335;">üìû </button>
        </div>
    </div>

    <audio id="aiAudio" preload="auto"></audio>
    {% csrf_token %}
    
    <!-- Professional Interview Audio System -->
    <script src="{% static 'js/professional-interview-audio.js' %}"></script>
    <script src="{% static 'js/speech-isolation-fix.js' %}"></script>

    <script>
        let recognition = null;
        let isListening = false;
        let questionCount = 1;
        let timeLeft = 15 * 60;
        let userStream = null;
        let isCameraOn = true;
        let audioContext = null;
        let microphoneSource = null;
        let echoCancellation = null;
        
        // Initialize camera with audio isolation
        async function initCamera() {
            try {
                // Request media with echo cancellation and noise suppression
                userStream = await navigator.mediaDevices.getUserMedia({ 
                    video: true, 
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true,
                        sampleRate: 44100
                    }
                });
                
                const videoElement = document.getElementById('userVideo');
                videoElement.srcObject = userStream;
                
                // Setup audio isolation to prevent feedback
                setupAudioIsolation();
                
            } catch (error) {
                console.error('Camera error:', error);
            }
        }
        
        // Setup audio isolation to prevent speaker feedback
        function setupAudioIsolation() {
            try {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                
                // Create microphone source
                microphoneSource = audioContext.createMediaStreamSource(userStream);
                
                // Create gain node for volume control
                const gainNode = audioContext.createGain();
                gainNode.gain.value = 1.0;
                
                // Connect microphone to gain node
                microphoneSource.connect(gainNode);
                
                console.log('Audio isolation setup complete');
            } catch (error) {
                console.error('Audio isolation setup failed:', error);
            }
        }
        
        // Toggle camera
        function toggleCamera() {
            if (!userStream) return;
            
            const videoTracks = userStream.getVideoTracks();
            isCameraOn = !isCameraOn;
            
            videoTracks.forEach(track => {
                track.enabled = isCameraOn;
            });
            
            const btn = document.getElementById('cameraBtn');
            if (isCameraOn) {
                btn.innerHTML = 'üìπ';
                btn.classList.remove('camera-off');
            } else {
                btn.innerHTML = 'üìπ';
                btn.classList.add('camera-off');
            }
        }
        
        // Update speech status indicator
        function updateSpeechStatus(status, message) {
            const statusElement = document.getElementById('speechStatus');
            if (!statusElement) return;
            
            statusElement.className = `speech-status ${status}`;
            statusElement.innerHTML = message;
        }
        
        // Show current question with streaming effect
        function showCurrentQuestion(question) {
            const element = document.getElementById('conversationArea');
            element.innerHTML = '';
            element.className = 'conversation-text ai-response';
            
            // Update status to show AI is speaking
            updateSpeechStatus('ai-speaking', 'ü§ñ AI Speaking...');
            
            let index = 0;
            const streamText = () => {
                if (index < question.length) {
                    element.innerHTML += question.charAt(index);
                    index++;
                    setTimeout(streamText, 15);
                } else {
                    // Question finished streaming, update status
                    setTimeout(() => {
                        updateSpeechStatus('candidate-ready', 'üé§ Ready for your response');
                    }, 1000);
                }
            };
            streamText();
        }
        
        // Show candidate response in same area after question disappears
        function showCandidateResponse(text) {
            const element = document.getElementById('conversationArea');
            element.innerHTML = '';
            element.className = 'conversation-text live-transcription';
            
            // Update status to show candidate is speaking
            updateSpeechStatus('processing', 'üó£Ô∏è Processing your response...');
            
            // Then stream the response
            let index = 0;
            const streamText = () => {
                if (index < text.length) {
                    element.innerHTML += text.charAt(index);
                    index++;
                    setTimeout(streamText, 20);
                }
            };
            streamText();
        }
        
        // Clear conversation area
        function clearConversation() {
            document.getElementById('conversationArea').innerHTML = '';
        }
        
        let collectedText = '';
        let speechTimeout = null;
        let lastSpeechTime = 0;
        let isProcessingResponse = false;
        
        // Initialize speech recognition with auto-start and auto-submit
        function initSpeech() {
            if ('webkitSpeechRecognition' in window) {
                recognition = new webkitSpeechRecognition();
                recognition.continuous = true;
                recognition.interimResults = true;
                recognition.lang = 'en-US';
                
                recognition.onstart = function() {
                    isListening = true;
                    collectedText = '';
                    document.getElementById('micBtn').classList.add('recording');
                    document.getElementById('micBtn').innerHTML = 'üî¥';
                    console.log('Microphone started automatically');
                };
                
                recognition.onend = function() {
                    if (isListening && !isProcessingResponse) {
                        // Auto-restart to keep microphone always on
                        setTimeout(() => {
                            if (isListening && !isProcessingResponse) {
                                try {
                                    recognition.start();
                                } catch (e) {
                                    console.log('Recognition restart failed:', e);
                                }
                            }
                        }, 100);
                    }
                };
                
                recognition.onerror = function(event) {
                    console.log('Speech recognition error:', event.error);
                    if (event.error === 'no-speech' || event.error === 'audio-capture') {
                        // Restart on common errors
                        setTimeout(() => {
                            if (isListening && !isProcessingResponse) {
                                try {
                                    recognition.start();
                                } catch (e) {
                                    console.log('Recognition restart after error failed:', e);
                                }
                            }
                        }, 1000);
                    }
                };
                
                recognition.onresult = function(event) {
                    // Ignore speech input while AI is speaking or processing
                    if (isProcessingResponse || window.professionalAudio?.aiAudioPlaying) {
                        console.log('üö´ Ignoring speech input - AI is currently speaking or processing');
                        return;
                    }
                    
                    let interimTranscript = '';
                    let finalTranscript = '';
                    
                    for (let i = event.resultIndex; i < event.results.length; i++) {
                        if (event.results[i].isFinal) {
                            finalTranscript += event.results[i][0].transcript;
                        } else {
                            interimTranscript += event.results[i][0].transcript;
                        }
                    }
                    
                    // Filter out potential AI speech that might have been picked up
                    const filteredFinalTranscript = filterAISpeech(finalTranscript);
                    const filteredInterimTranscript = filterAISpeech(interimTranscript);
                    
                    // Collect all text (only if it's likely candidate speech)
                    if (filteredFinalTranscript && filteredFinalTranscript.trim()) {
                        collectedText += filteredFinalTranscript;
                        lastSpeechTime = Date.now();
                        
                        console.log('‚úÖ Candidate speech detected:', filteredFinalTranscript);
                        
                        // Clear any existing timeout
                        if (speechTimeout) {
                            clearTimeout(speechTimeout);
                        }
                        
                        // Set timeout to auto-submit after 1 second of silence (increased for better accuracy)
                        speechTimeout = setTimeout(() => {
                            if (collectedText.trim() && !isProcessingResponse) {
                                autoSubmitResponse();
                            }
                        }, 1000);
                    }
                    
                    // Show live text with interim results (only filtered content)
                    const displayText = collectedText + filteredInterimTranscript;
                    if (displayText.trim()) {
                        document.getElementById('conversationArea').innerHTML = displayText;
                        
                        // Update last speech time for interim results too
                        if (filteredInterimTranscript.trim()) {
                            lastSpeechTime = Date.now();
                        }
                    }
                };
                
                // Start microphone automatically
                startMicrophone();
            }
        }
        
        // Filter out potential AI speech from candidate input
        function filterAISpeech(text) {
            if (!text || !text.trim()) return text;
            
            // Common AI interviewer phrases that should be filtered out
            const aiPhrases = [
                'hello', 'hi there', 'welcome', 'thanks for joining',
                'tell me about yourself', 'can you describe', 'what interests you',
                'that\'s great', 'excellent', 'interesting', 'perfect',
                'next question', 'moving on', 'let\'s talk about',
                'thank you for', 'any questions', 'do you have',
                'alex', 'interviewer', 'ai interviewer'
            ];
            
            const lowerText = text.toLowerCase();
            
            // If the text contains multiple AI phrases, it's likely AI speech
            let aiPhraseCount = 0;
            for (const phrase of aiPhrases) {
                if (lowerText.includes(phrase)) {
                    aiPhraseCount++;
                }
            }
            
            // If more than 2 AI phrases detected, filter it out
            if (aiPhraseCount >= 2) {
                console.log('üö´ Filtered out potential AI speech:', text);
                return '';
            }
            
            // Also filter out very short responses that might be audio artifacts
            if (text.trim().length < 3) {
                return '';
            }
            
            return text;
        }
        
        // Auto-submit response when candidate stops speaking
        function autoSubmitResponse() {
            if (isProcessingResponse || !collectedText.trim()) return;
            
            console.log('Auto-submitting response after speech pause');
            isProcessingResponse = true;
            
            // Clear the timeout
            if (speechTimeout) {
                clearTimeout(speechTimeout);
                speechTimeout = null;
            }
            
            // Send the response
            const responseText = collectedText.trim();
            collectedText = ''; // Clear for next response
            
            sendResponse(responseText).finally(() => {
                // Reset processing flag with a small delay to prevent immediate re-triggering
                setTimeout(() => {
                    isProcessingResponse = false;
                }, 500);
            });
        }
        
        // Start microphone automatically
        function startMicrophone() {
            if (recognition && !isListening) {
                try {
                    isListening = true;
                    recognition.start();
                    console.log('Microphone started automatically for professional interview');
                } catch (e) {
                    console.log('Failed to start microphone:', e);
                }
            }
        }
        
        // Toggle microphone (only allows manual turn off, auto-restarts)
        function toggleMic() {
            if (!recognition) return;
            
            if (isListening) {
                // Only allow manual turn off, but warn user
                if (confirm('Are you sure you want to turn off the microphone? This is not recommended during an interview.')) {
                    isListening = false;
                    isProcessingResponse = true;
                    recognition.stop();
                    document.getElementById('micBtn').classList.remove('recording');
                    document.getElementById('micBtn').innerHTML = 'üé§';
                    
                    // Clear any pending timeouts
                    if (speechTimeout) {
                        clearTimeout(speechTimeout);
                        speechTimeout = null;
                    }
                    
                    // Send the collected text to server if any
                    if (collectedText.trim()) {
                        sendResponse(collectedText.trim());
                        collectedText = '';
                    }
                    
                    // Show warning message
                    showWarningMessage('Microphone turned off. Click the microphone button to turn it back on.');
                }
            } else {
                // Restart listening
                isListening = true;
                isProcessingResponse = false;
                collectedText = '';
                try {
                    recognition.start();
                    hideWarningMessage();
                } catch (e) {
                    console.log('Failed to restart microphone:', e);
                }
            }
        }
        
        // Show warning message
        function showWarningMessage(message) {
            const warningDiv = document.createElement('div');
            warningDiv.id = 'micWarning';
            warningDiv.className = 'mic-warning';
            warningDiv.innerHTML = `üö® ${message}`;
            document.body.appendChild(warningDiv);
        }
        
        // Hide warning message
        function hideWarningMessage() {
            const warning = document.getElementById('micWarning');
            if (warning) {
                warning.remove();
            }
        }
        
        // Send response to server
        async function sendResponse(text) {
            try {
                console.log('Sending response:', text);
                
                const response = await fetch(window.location.href, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/x-www-form-urlencoded',
                        'X-CSRFToken': document.querySelector('[name=csrfmiddlewaretoken]').value,
                    },
                    body: new URLSearchParams({ text: text })
                });
                
                const data = await response.json();
                
                if (data.success) {
                    // Store AI response for speech filtering
                    if (window.speechIsolationManager) {
                        window.speechIsolationManager.setLastAIResponse(data.response);
                    }
                    
                    // Show new question immediately (no delay)
                    showCurrentQuestion(data.response);
                    
                    // Update counter
                    questionCount = data.question_count || questionCount + 1;
                    document.getElementById('questionCount').textContent = questionCount + '/8';
                    
                    // Play audio if available (with enhanced feedback prevention)
                    if (data.audio) {
                        const audio = document.getElementById('aiAudio');
                        audio.src = data.audio;
                        audio.volume = 0.6; // Further reduced volume to prevent feedback
                        
                        // Set processing flag to prevent speech recognition during AI speech
                        isProcessingResponse = true;
                        
                        // Update visual status
                        updateSpeechStatus('ai-speaking', 'üîä AI Speaking - Please Listen');
                        
                        console.log('üîä AI speaking - speech recognition disabled');
                        
                        // Handle audio end with proper cleanup
                        audio.addEventListener('ended', () => {
                            console.log('üîä AI audio ended');
                            setTimeout(() => {
                                isProcessingResponse = false;
                                updateSpeechStatus('candidate-ready', 'üé§ Ready for your response');
                                console.log('‚úÖ Ready for candidate response');
                            }, 1000); // Increased delay to ensure clean separation
                        }, { once: true });
                        
                        // Handle audio errors
                        audio.addEventListener('error', () => {
                            console.log('‚ùå AI audio error - resetting processing flag');
                            isProcessingResponse = false;
                        }, { once: true });
                        
                        audio.play().catch(e => {
                            console.log('Audio play failed:', e);
                            isProcessingResponse = false;
                        });
                    } else {
                        // No audio, ready for next response after a short delay
                        setTimeout(() => {
                            isProcessingResponse = false;
                            updateSpeechStatus('candidate-ready', 'üé§ Ready for your response');
                            console.log('‚úÖ Ready for next candidate response (no audio)');
                        }, 500);
                    }
                    
                    // Check if complete
                    if (data.is_final || questionCount >= 8) {
                        setTimeout(endInterview, 3000);
                    }
                }
            } catch (error) {
                console.error('Send error:', error);
                isProcessingResponse = false; // Reset on error
            }
        }
        
        // Timer
        function updateTimer() {
            const minutes = Math.floor(timeLeft / 60);
            const seconds = timeLeft % 60;
            document.getElementById('timer').textContent = 
                minutes + ':' + seconds.toString().padStart(2, '0');
            
            if (timeLeft <= 0) {
                endInterview();
                return;
            }
            timeLeft--;
        }
        
        // End interview
        function endInterview() {
            if (recognition && isListening) {
                isListening = false;
                recognition.stop();
            }
            alert('Interview completed! Redirecting to dashboard...');
            window.location.href = '{% url "jobseeker_dashboard" %}';
        }
        
        // Mute AI audio to prevent feedback
        function muteAIAudio() {
            const aiAudio = document.getElementById('aiAudio');
            if (aiAudio) {
                aiAudio.addEventListener('play', function() {
                    // Temporarily pause speech recognition during AI speech
                    if (recognition && isListening) {
                        try {
                            recognition.stop();
                            setTimeout(() => {
                                if (isListening) {
                                    recognition.start();
                                }
                            }, aiAudio.duration * 1000 + 500); // Resume after audio ends
                        } catch (e) {
                            console.log('Audio isolation error:', e);
                        }
                    }
                });
            }
        }
        
        // Initialize everything
        document.addEventListener('DOMContentLoaded', function() {
            initCamera();
            initSpeech();
            muteAIAudio();
            setInterval(updateTimer, 1000);
            
            // Clear conversation area initially
            clearConversation();
            
            // Show initial question with streaming effect
            const initialQuestion = '{{ ai_question|default:"Hello! Welcome to your AI interview. Can you tell me about yourself?" }}';
            
            // Store initial AI question for speech filtering
            if (window.speechIsolationManager) {
                window.speechIsolationManager.setLastAIResponse(initialQuestion);
            }
            
            showCurrentQuestion(initialQuestion);
            
            // Show professional interview notice
            showProfessionalNotice();
            
            // Play initial audio if available (with enhanced isolation)
            const initialAudio = '{{ audio_url|safe }}';
            if (initialAudio && initialAudio !== 'None' && initialAudio !== '') {
                setTimeout(() => {
                    const audio = document.getElementById('aiAudio');
                    audio.src = initialAudio;
                    audio.volume = 0.6; // Further reduced volume to prevent feedback
                    
                    // Set processing flag during initial audio
                    isProcessingResponse = true;
                    
                    audio.addEventListener('ended', () => {
                        setTimeout(() => {
                            isProcessingResponse = false;
                            updateSpeechStatus('candidate-ready', 'üé§ Ready for your response');
                            console.log('‚úÖ Initial audio ended - ready for candidate response');
                        }, 1000);
                    }, { once: true });
                    
                    audio.play().catch(e => {
                        console.log('Initial audio failed:', e);
                        isProcessingResponse = false;
                    });
                }, 1500); // Increased delay to ensure speech recognition is fully initialized
            }
        });
        
        // Show professional interview notice
        function showProfessionalNotice() {
            const notice = document.createElement('div');
            notice.className = 'professional-notice';
            notice.innerHTML = 'üé§ Professional Interview Mode: Microphone is ALWAYS ON';
            document.body.appendChild(notice);
            
            // Remove notice after 5 seconds
            setTimeout(() => {
                notice.style.animation = 'slideUp 0.5s ease-out forwards';
                setTimeout(() => notice.remove(), 500);
            }, 5000);
        }
        
        // Update audio level visualization
        function updateAudioLevelIndicator(level = 0) {
            const indicator = document.getElementById('audioLevelIndicator');
            if (!indicator) return;
            
            const bars = indicator.querySelectorAll('.audio-level-bar');
            const activeCount = Math.floor((level / 100) * bars.length);
            
            bars.forEach((bar, index) => {
                if (index < activeCount) {
                    bar.classList.add('active');
                } else {
                    bar.classList.remove('active');
                }
            });
        }
        
        // Handle speech result with enhanced filtering and UI
        function handleSpeechResult(event) {
            // Double-check that AI is not speaking
            if (isProcessingResponse || window.professionalAudio?.aiAudioPlaying) {
                console.log('üö´ Speech result ignored - AI is speaking');
                return;
            }
            
            let interimTranscript = '';
            let finalTranscript = '';
            
            for (let i = event.resultIndex; i < event.results.length; i++) {
                if (event.results[i].isFinal) {
                    finalTranscript += event.results[i][0].transcript;
                } else {
                    interimTranscript += event.results[i][0].transcript;
                }
            }
            
            // Filter out potential AI speech
            const filteredFinalTranscript = filterAISpeech(finalTranscript);
            const filteredInterimTranscript = filterAISpeech(interimTranscript);
            
            // Collect all text and handle auto-submission (only filtered content)
            if (filteredFinalTranscript && filteredFinalTranscript.trim()) {
                collectedText += filteredFinalTranscript;
                lastSpeechTime = Date.now();
                
                console.log('‚úÖ Valid candidate speech collected:', filteredFinalTranscript);
                
                // Clear any existing timeout
                if (speechTimeout) {
                    clearTimeout(speechTimeout);
                }
                
                // Set timeout to auto-submit after 1 second of silence (increased for accuracy)
                speechTimeout = setTimeout(() => {
                    if (collectedText.trim() && !isProcessingResponse) {
                        autoSubmitResponse();
                    }
                }, 1000);
            }
            
            // Show live text with enhanced styling (only filtered content)
            const displayText = collectedText + filteredInterimTranscript;
            const conversationArea = document.getElementById('conversationArea');
            if (displayText.trim() && conversationArea) {
                conversationArea.innerHTML = displayText;
                conversationArea.className = 'conversation-text live-transcription';
                
                // Update status to show candidate is speaking
                updateSpeechStatus('processing', 'üó£Ô∏è Listening to your response...');
                
                // Simulate audio level based on speech activity
                updateAudioLevelIndicator(filteredInterimTranscript ? 60 : 20);
                
                // Update last speech time for interim results too
                if (filteredInterimTranscript.trim()) {
                    lastSpeechTime = Date.now();
                }
            }
        }
        
        // Make handleSpeechResult globally available
        window.handleSpeechResult = handleSpeechResult;
    </script>
</body>
</html>